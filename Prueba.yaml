permanent_config: '/home/ddelahoz/Desarrollo/HDD/Opennmt/onmt-evaluation/permanent_config.yaml'
translation_config:
    gpu: 0
    batch_size: 16384 
    batch_type: tokens 
    beam_size: 5 
    max_length: 300 
    replace_unk: True

evaluations:
    evaluation_1:
        evaluation_name: newstest-2013
        datasets: [a, b, c]
        language-pair: en-es
        tokenizer_config:
        #This section must have a complete alingment with tokenizer parameters
            -   mode: conservative
                bpe_model_path:
                joiner_annotate: 
        models: [1, 2, 3]
        metrics: [1, 2, 3]
        save_directory: Prueba
    evaluation_2:
        evaluation_name:
        datasets: [a, b, c]
        language-pair: 
        tokenizer:
        models: []
        metrics: []
        save_results:

# save_data: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/final_models/internal_internal_tokenizer
# ## Where the vocab(s) will be written 
# ## WARNING: THIS VOCAB IS GENERATED BY THE 'onmt_build_vocab' COMMAND, NOT THE ONE GENERATED BY SENTENCEPIECE.
# src_vocab: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/internal_corpus/internal_en-es.vocab
# share_vocab: True

# # Corpus opts:
# data:
#     # tatoeba:
#     #     path_src: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/tatoeba_corpus/data/en-es/train.trg
#     #     path_tgt: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/tatoeba_corpus/data/en-es/train.src
#     #     transforms: [onmt_tokenize, filtertoolong]
#     #    weight: 950
#     internal:
#         path_src: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/internal_corpus/en-es/general/data/Pruebas/TRAINING.es
#         path_tgt: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/internal_corpus/en-es/general/data/Pruebas/TRAINING.en
#         transforms: [onmt_tokenize, filtertoolong]
#         weight: 50
#     valid:
#         path_src: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/tatoeba_corpus/data/en-es/valid.es
#         path_tgt: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/tatoeba_corpus/data/en-es/valid.en
#         transforms: [onmt_tokenize]

# ### Transform related opts:
# #### Subword
# src_subword_model: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/internal_corpus/internal-enes-32k.model
# src_subword_type: bpe
# src_subword_nbest: 1
# src_subword_alpha: 0.0
# src_onmttok_kwargs: "{'mode': 'conservative', 'joiner_annotate' : True, 'case_markup' : True, 'soft_case_regions' : True}"

# tgt_subword_model: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/internal_corpus/internal-enes-32k.model
# tgt_subword_type: bpe
# tgt_subword_nbest: 1
# tgt_subword_alpha: 0.0
# tgt_onmttok_kwargs: "{'mode': 'conservative', 'joiner_annotate' : True, 'case_markup' : True, 'soft_case_regions' : True}"

# #### Filter
# #src_seq_length: 150

# # silently ignore empty lines in the /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/data
# skip_empty_level: silent

# #General opts
# #train_from: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/training_continuation/run/model_step_560000.pt
# save_model: /lustre/ific.uv.es/ml/upm044/0443/Pruebas/OpenNMT_py/final_models/mixed/run/model
# save_checkpoint_steps: 10000
# keep_checkpoint: 100
# seed: 3435
# train_steps: 550000
# valid_steps: 10000
# warmup_steps: 8000
# report_every: 100
# position_encoding: True

# # Batching
# queue_size: 10000
# bucket_size: 32768
# world_size: 4
# gpu_ranks: [0, 1, 2, 3]
# batch_type: "tokens"
# batch_size: 4096
# valid_batch_size: 4096
# batch_size_multiple: 1
# max_generator_batches: 2
# accum_count: [1]
# accum_steps: [0]

# # Optimization
# model_dtype: "fp32"
# optim: "adam"
# learning_rate: 2
# warmup_steps: 8000
# decay_method: "noam"
# adam_beta2: 0.998
# max_grad_norm: 0
# label_smoothing: 0.1
# param_init: 0
# param_init_glorot: true
# normalization: "tokens"

# # Model
# encoder_type: transformer
# decoder_type: transformer
# enc_layers: 6
# dec_layers: 6
# heads: 16
# rnn_size: 512
# word_vec_size: 512
# transformer_ff: 2048
# dropout_steps: [0]
# dropout: [0.1]
# attention_dropout: [0.1]
# share_decoder_embeddings: true
# share_embeddings: true